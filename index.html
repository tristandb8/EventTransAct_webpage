<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EventTransAct: A video transformer-based framework for Event-camera based action recognition</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/IROS-2023 (1).png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.crcv.ucf.edu/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EventTransAct: A video transformer-based framework for Event-camera based action recognition</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a >Tristan De Blegiers</a><sup>*</sup>,</span>
            <span class="author-block">
              <a >Ishan Rajendrakumar Dave</a><sup>*</sup>,</span>
            <span class="author-block">
              <a >Adeel Yousaf</a>,
            </span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/"> Mubarak Shah</a>,
            </span>
            <!-- <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Center for Research in Computer Vision (CRCV), University of Central Florida</span><br>
            <span class="author-block" style="font-size: 0.5em;"><sup>*</sup> denotes equal contribution</span>


            
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2308.13711.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2308.13711"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=YCff-rTrgco&t=187s"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tristandb8/EventTransAct"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/EgocentricVision/N-EPIC-Kitchens"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recognizing and comprehending human actions and gestures is a crucial perception requirement for robots to interact with humans and carry out tasks in diverse domains, including service robotics, healthcare, and manufacturing.Event cameras, with their ability to capture fast-moving objects at a high temporal resolution, offer new opportunities compared to standard action recognition in RGB videos. However, previous research on event camera action recognition has primarily focused on sensor-specific network architectures and image encoding, which may not be suitable for new sensors and limit the use of recent advancement in transformer-based architectures.
          </p>
          <p>
            In this study, we employ using a computationally efficient model, namely the video transformer network (VTN), which initially acquires spatial embeddings per event- frame and then utilizes a temporal self-attention mechanism. This approach separates the spatial and temporal operations, resulting in VTN being more computationally efficient than other video transformers that process spatio-temporal volumes directly. In order to better adopt the VTN for the sparse and finegrained nature of event data, we design Event-Contrastive Loss (LEC) and event specific augmentations. Proposed LEC promotes learning fine-grained spatial cues in the spatial backbone of VTN by contrasting temporally misaligned frames.
          </p>
          <p>
            We evaluate our method on real-world action recognition of N-EPIC Kitchens dataset, and achieve state-of-the-art results on both protocols - testing in seen kitchen (74.9% accuracy) and testing in unseen kitchens (42.43% and 46.66% Accuracy). Our approach also takes less computation time compared to competitive prior approaches. We also evaluate our method on the standard DVS Gesture recognition dataset, achieving a competitive accuracy of 97.9% compared to prior work that uses dedicated architectures and image-encoding for the DVS dataset. These results demonstrate the potential of our framework EventTransAct for real-world applications of event-camera based action recognition
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

 <!-- Figure1. -->
 <section class="section">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Event Contrastive Learning</h2>

      <!-- Interpolating. -->
      <!-- <h3 class="title is-4">Interpolating states</h3> -->
      <div class="content has-text-justified">
        <p>
          Event Contrastive Learning increases temporal-distinctiveness of the spatial embedding F(x) by maximizing the agreement between two differently augmented versions of the same frame, whereas it maximizes the disagreement between temporally misaligned frames. For visualization purpose, only 3 frames per clip are shown.
        </p>
      </div>
      <!-- <div class="columns is-hcentered interpolation-panel"> -->
        <!-- <div class="column is-3 has-text-centered"> -->
      <img src="./static/images/image.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
          <!-- style="width:800px;height:500px"/> -->
      <!-- <p>Figure1</p> -->
        <!-- </div> -->
   
      <!-- </div> -->
      

    </div>
  </div>



  <section class="section">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
  
        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <div class="content has-text-justified">
          <p>
            We employed a Video Transformer Network (VTN) architecture [17], which expands an image encoder F with a transformer E that uses frame embeddings as input tokens. Each event-frame x(i) is passed through a spatial-encoder F(·) to extract spatial features, which are then processed by a transformer-based LongFormer module E(·), augmented with positional embeddings (PE) and a [CLS] token, to learn global temporal dependencies. The [CLS] token output is then classified by head G(·). The model is trained using both cross-entropy loss and a proposed event contrastive loss.
          </p>
        </div>
        <!-- <div class="columns is-hcentered interpolation-panel"> -->
          <!-- <div class="column is-3 has-text-centered"> -->
        <img src="./static/images/figure_2.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
            <!-- style="width:800px;height:500px"/> -->
        <!-- <p>Figure1</p> -->
          <!-- </div> -->
     
        <!-- </div> -->
        
  
      </div>
    </div>

    
<section class="section">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Results</h2>

      <!-- Interpolating. -->
      <!-- <h3 class="title is-4">Interpolating states</h3> -->
      <div class="content has-text-justified">
        <p>
          Our experimental results demonstrate the effectiveness of the proposed method for both gesture and real-world action recognition tasks. Particularly, the state-of-the-art performance on the N-EPIC Kitchens dataset, where the model is evaluated on unseen environments, highlights its ability to generalize to novel settings, making it a more reliable choice for deployment.
        </p>
      </div>
      <!-- <div class="columns is-hcentered interpolation-panel"> -->
        <!-- <div class="column is-3 has-text-centered"> -->
      <img src="./static/images/results.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
          <!-- style="width:800px;height:500px"/> -->
      <!-- <p>Figure1</p> -->
        <!-- </div> -->
    
      <!-- </div> -->
      

    </div>
  </div>    

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Video</h2>
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/event_camera.mp4"
                type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2> -->
    </div>
  </div>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we proposed a video transformer-based framework for event-camera based action recognition, which leverages event-contrastive loss and augmentations to adapt the network to event data. Our method achieved state-of- the-art results on N-EPIC Kitchens dataset and competitive results on standard DVS Gesture recognition datase, while requiring less computation time compared to competitive prior approaches. Our findings demonstrate the effectiveness of our proposed framework and highlight its potential impact in real-world applications.
          </p>
          <p>
            Future research in this area could extend our work to Action Quality Assessment tasks that require more fine-grained temporal understanding than Action Recognition task, making them more relevant for event-based cameras. Another interesting direction could be exploring recent masked image modeling-based learning techniques to efficiently adapt RGB models to event data. Overall, we believe that our work contributes to the advancement of event-based video understanding and provides a strong foundation for future research in this area.
          </p>
       </div>
      </div>
    </div>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{Blegiers2023EventTransAct,
  author    = {Blegiers,Tristan and Dave, Ishan and Yousaf, Adeel and Shah, Mubarak},
  title     = {EventTransAct: A video transformer-based framework for Event-camera based action recognition},
  conference   = {IROS},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
